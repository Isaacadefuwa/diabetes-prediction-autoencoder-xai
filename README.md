# Anomaly-Based Diabetes Prediction Using Autoencoder and Explainable AI (XAI)
An unsupervised learning model for early diabetes detection using Autoencoder and SHAP for explainability.

This project presents a deep learning-based anomaly detection model that leverages Autoencoders and SHAP (Shapley Additive Explanations) for interpreting predictions in early diabetes diagnosis.

## 📊 Dataset
- Diabetes Prediction Dataset ( Sourced From Kaggle)
  
## 🧪 Preprocessing
- One-hot encoding
- MinMax normalization
- Class balancing using SMOTE
- Feature selection and scaling
  
## 🛠️ Model & Tools
- Autoencoder (unsupervised)
- Hyperparameter tuning with Hyperopt
- SHAP for explainability
- Tools: Python, pandas, scikit-learn, Keras, SHAP, SMOTE

  ## 📈 Results
- Accuracy: 90.92%
- Precision: 49.56%
- Recall: 88.71%
- F1-score: 63.59%
- ROC-AUC: 90%

The model effectively flagged anomalous data points likely to represent diabetic cases.
SHAP analysis revealed that smoking status, BMI, and glucose level were among the most influential features.

## 🔍 Key Features
- Interpretable AI using SHAP
- Balanced training with SMOTE
- Tuned model architecture via Hyperopt

## ✅ What I Learned
Applied unsupervised learning in a real-world health scenario
Used SHAP for transparent, explainable AI
Improved confidence in Python model building, tuning, and visualization





